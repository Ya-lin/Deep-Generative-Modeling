{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3882ed68-f063-44b9-bf45-645eec858876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pdb import set_trace\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "import keras\n",
    "from keras import layers, models\n",
    "from dataset import get_fmnist\n",
    "from network import get_encoder, get_decoder, AE\n",
    "from train import trainer\n",
    "from test import reconstruction, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227262c4-2b15-45b6-ab9a-259947cdf280",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3b1a5e-abda-43d9-b781-7931e30e7623",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path.home().joinpath(\"Documents\",\"Data\")\n",
    "path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527d25dc-2841-4224-9793-8a6035ba1143",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = get_fmnist(path, 100)\n",
    "len(loader[0]),len(loader[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a45341b-84c9-4a96-9c59-d8024810057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(loader[0]))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075ab08e-c92d-41e6-93d8-acff1fa52eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "channels = 1\n",
    "batch_size = 100\n",
    "embedding_dim = 2\n",
    "epochs = 10\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941772aa-01d5-49cf-8513-0ecb4c1201a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder, shape_bf = get_encoder(image_size, channels, embedding_dim)\n",
    "print(shape_bf)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da392d1-59e1-4cc0-af19-8075404d8a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "decoder = get_decoder(embedding_dim, channels, shape_bf)\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c4ddff-e74d-4a30-8985-7a706395f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AE(image_size, channels, embedding_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c2774-ba85-47be-91f3-e64a58cf7e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = trainer(model, loader, epochs, \n",
    "                         optimizer, loss_fn, device)\n",
    "model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca2c6ac-d29c-4935-b224-a1072e500015",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history[\"train\"], label=\"train loss\")\n",
    "plt.plot(history[\"test\"], label=\"test loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59271cca-e57b-4dbf-af54-b89be1cf6a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved model\n",
    "checkpoint = torch.load(\"best_model.pth\", weights_only=True)\n",
    "model.load_state_dict(checkpoint[\"model_dict\"])\n",
    "model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05fad6d-833a-44cb-a97d-aafdcc8627f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_hat, Y = reconstruction(model, loader[1], 50, device)\n",
    "X.shape, X_hat.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe445e-cbe7-491c-a76d-bc7c3256d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example real clothing items\")\n",
    "display(X)\n",
    "print(\"Correpsonding reconstructions\")\n",
    "display(X_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4761f7d4-e9f2-462f-8cb3-07ee546a08ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the example images\n",
    "with torch.no_grad():\n",
    "    embeddings = model.encoder(X).cpu().numpy()\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5084729-1a6a-4d2f-8155-7875f9421a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colour the embeddings by their label (clothing type - see table)\n",
    "figsize = 8\n",
    "plt.figure(figsize=(figsize, figsize))\n",
    "plt.scatter(embeddings[:, 0], embeddings[:, 1], \n",
    "            cmap=\"rainbow\", c=Y, alpha=0.8, s=3)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8843c2f0-de77-4c48-8040-97b0c90a371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the range of the existing embeddings\n",
    "mins, maxs = np.min(embeddings, axis=0), np.max(embeddings, axis=0)\n",
    "\n",
    "# Sample some points in the latent space\n",
    "grid_width, grid_height = (6, 3)\n",
    "sample = np.random.uniform(mins, maxs, \n",
    "         size=(grid_width*grid_height,embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccee94e-0f0d-4e0a-9c47-e2e87f3c5f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the sampled points\n",
    "with torch.no_grad():\n",
    "    generated_sample = model.decoder(sample).cpu().numpy()\n",
    "generated_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc551ef-12d8-4887-a4e0-35217c6a98b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a plot of...\n",
    "figsize = 8\n",
    "plt.figure(figsize=(figsize, figsize))\n",
    "\n",
    "# ... the original embeddings ...\n",
    "plt.scatter(embeddings[:, 0], embeddings[:, 1], c=\"black\", alpha=0.5, s=2)\n",
    "\n",
    "# ... and the newly generated points in the latent space\n",
    "plt.scatter(sample[:, 0], sample[:, 1], c=\"#00B0F0\", alpha=1, s=40)\n",
    "plt.show()\n",
    "\n",
    "# Add underneath a grid of the decoded images\n",
    "fig = plt.figure(figsize=(figsize, grid_height * 2))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "for i in range(grid_width * grid_height):\n",
    "    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n",
    "    ax.axis(\"off\")\n",
    "    ax.text(0.5, -0.35, str(np.round(sample[i, :], 1)),\n",
    "            fontsize=10, ha=\"center\", transform=ax.transAxes)\n",
    "    ax.imshow(generated_sample[i, :, :], cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fa0dbb-645c-4f06-8d4a-a478e4c7c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colour the embeddings by their label (clothing type - see table)\n",
    "figsize = 12\n",
    "grid_size = 15\n",
    "plt.figure(figsize=(figsize, figsize))\n",
    "plt.scatter(embeddings[:, 0], embeddings[:, 1], cmap=\"rainbow\",\n",
    "            c=Y, alpha=0.8, s=300)\n",
    "plt.colorbar()\n",
    "\n",
    "x = np.linspace(min(embeddings[:, 0]), max(embeddings[:, 0]), grid_size)\n",
    "y = np.linspace(max(embeddings[:, 1]), min(embeddings[:, 1]), grid_size)\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "xv = xv.flatten()\n",
    "yv = yv.flatten()\n",
    "grid = np.array(list(zip(xv, yv)))\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_sample = model.decoder(grid).cpu().numpy()\n",
    "# plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(figsize, figsize))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "for i in range(grid_size**2):\n",
    "    ax = fig.add_subplot(grid_size, grid_size, i + 1)\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(generated_sample[i, :, :], cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad233a6b-ae05-4881-9ebf-a52d133832ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
